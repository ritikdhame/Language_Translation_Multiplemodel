{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "G9ZhLxbYWgdM"
      },
      "source": [
        "# Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3dc3-4wWgdO",
        "outputId": "0fa4db77-1643-49b9-c758-6a7082c0ecf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uGPKzMUWYqDzPox6GznsT1Hx4ERpiAMo\n",
            "To: /content/Hindi_English_Truncated_Corpus.csv\n",
            "100%|██████████| 41.4M/41.4M [00:00<00:00, 90.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "import re\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import io\n",
        "import time\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "data_file_1 = 'Hindi_English_Truncated_Corpus.csv'\n",
        "\n",
        "# Check if the file exists in the current directory\n",
        "if not os.path.isfile(data_file_1):\n",
        "    # Download the data file from Google Drive\n",
        "    url = 'https://drive.google.com/uc?id=1uGPKzMUWYqDzPox6GznsT1Hx4ERpiAMo'\n",
        "    output = data_file_1\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "lines = pd.read_csv(data_file_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "if tf.executing_eagerly():\n",
        "    print(\"Eager execution is enabled.\")\n",
        "else:\n",
        "    print(\"Eager execution is not enabled.\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4dKKcLCYBAy",
        "outputId": "5e4bfc2f-f1a4-47a5-a536-64d922d360dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eager execution is enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/Hindi_English_Truncated_Corpus.csv\""
      ],
      "metadata": {
        "id": "Kq2BQpJqYOT4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2rWkVloWgdP"
      },
      "source": [
        "## Preprocess English and Hindi sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lRHdpW_6WgdQ"
      },
      "outputs": [],
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    return w\n",
        "\n",
        "def hindi_preprocess_sentence(w):\n",
        "    w = w.rstrip().strip()\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ErEhkWG_WgdQ"
      },
      "outputs": [],
      "source": [
        "def create_dataset(path=PATH):\n",
        "    lines=pd.read_csv(path,encoding='utf-8')\n",
        "    lines=lines.dropna()\n",
        "    lines = lines[lines['source']=='ted']\n",
        "    en = []\n",
        "    hd = []\n",
        "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n",
        "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
        "        en_1.append('<end>')\n",
        "        en_1.insert(0, '<start>')\n",
        "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
        "        hd_1.append('<end>')\n",
        "        hd_1.insert(0, '<start>')\n",
        "        en.append(en_1)\n",
        "        hd.append(hd_1)\n",
        "    return hd, en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I04sM928WgdR"
      },
      "outputs": [],
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqgt4A5gWgdR"
      },
      "source": [
        "### Tokenization of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "82h2s2dcWgdR"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "64ZUoASQWgdS"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path=PATH):\n",
        "    targ_lang, inp_lang = create_dataset(path)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0im4lDvyWgdS"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttWClc1bWgdS"
      },
      "source": [
        "### Create Train and Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yRYz3GinWgdS",
        "outputId": "6f2024d6-7366-4ad2-c86c-1205adc2fe6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31904 31904 7977 7977\n"
          ]
        }
      ],
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U6WSpf0uWgdS",
        "outputId": "c8c6d94a-c600-4856-efa8-33ba96bcd838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "39 ----> do\n",
            "12 ----> you\n",
            "81 ----> know\n",
            "9 ----> that\n",
            "5785 ----> bipolar\n",
            "2704 ----> disorder\n",
            "11 ----> is\n",
            "11103 ----> nicknamed\n",
            "3 ----> the\n",
            "5968 ----> ceo\n",
            "8511 ----> disease ?\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "41 ----> क्या\n",
            "69 ----> आपको\n",
            "108 ----> पता\n",
            "6 ----> है\n",
            "10 ----> कि\n",
            "7883 ----> बाईपोलर\n",
            "5747 ----> डिसार्डर\n",
            "11 ----> को\n",
            "10937 ----> सी.ई.ओ.\n",
            "828 ----> बीमारी\n",
            "26 ----> भी\n",
            "212 ----> कहते\n",
            "13 ----> हैं\n",
            "203 ----> ?\n",
            "2 ----> <end>\n"
          ]
        }
      ],
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RoB76QxWgdT"
      },
      "source": [
        "### Create Dataset\n",
        "> We are using minimal configuration as the notebbok is not focussed on metrics performance but rather the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uXihcUnOWgdT"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LOIhnGZWgdT"
      },
      "source": [
        "## Encoder Decoder with Attention Model\n",
        "\n",
        "> The Encoder-Decoder model with Attention is a versatile approach to sequence learning that doesn't rely heavily on pre-defined assumptions about the structure of the sequences. It leverages a multilayered Gated Recurrent Unit (GRU) to transform the input sequence into a fixed-sized vector representation. Subsequently, another deep GRU is used to generate the target sequence from this vector representation.\n",
        "<img src=\"https://www.researchgate.net/profile/Vlad_Zhukov2/publication/321210603/figure/fig1/AS:642862530191361@1530281779831/An-example-of-sequence-to-sequence-model-with-attention-Calculation-of-cross-entropy.png\" width=\"800\" alt=\"attention mechanism\">\n",
        "\n",
        ">In a sequence-to-sequence model, there are two components: an encoder and a decoder. These components are essentially two distinct neural network models combined into a single large network. The role of the encoder is to comprehend the input sequence and generate a lower-dimensional representation of it. This representation is then passed to the decoder, which produces its own sequence as the output.\n",
        "\n",
        "The input sequence is fed into the encoder model, which produces the encoder output. To capture the importance of each input word, the attention mechanism assigns weights to them. These weights are then utilized by the decoder to predict the next word in the sentence. In this particular case, the Bahdanau attention mechanism is employed for the encoder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk1jWvfKWgdT"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gFEeGFFfWgdT"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB980faYWgdT"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aMn2psB-WgdT"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLJlXe7tWgdU"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oRVOCfjsWgdU"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV-4UIzNWgdU"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qUqTWcfeWgdU"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#   print(type(mask))\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7UU9hpIdWgdU"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS5mZM6nWgdU"
      },
      "source": [
        "## Training\n",
        "\n",
        ">1. Pass *input* through *encoder* to get *encoder output*..\n",
        ">2. Then encoder output, encoder hidden state and the decoder input is passed to decoder.\n",
        ">3. Decoder returns *predictions* and *decoder hidden state*.\n",
        ">4. Decoder hidden state is then passed back to model.\n",
        ">5. Predictions are used to calculate loss.\n",
        ">6. Use *teacher forcing* (technique where the target word is passed as the next input to the decoder) for the next input to the decoder.\n",
        ">7. Calculate gradients and apply it to *optimizer* for backpropogation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F2yJhI_NWgdU"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_VOz5QhUWgdU",
        "outputId": "4a5ca483-ae4e-4632-c98c-42589435f19d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.8985\n",
            "Epoch 1 Batch 100 Loss 1.7292\n",
            "Epoch 1 Batch 200 Loss 1.8486\n",
            "Epoch 1 Batch 300 Loss 1.7851\n",
            "Epoch 1 Batch 400 Loss 1.8360\n",
            "Epoch 1 Loss 1.9441\n",
            "Time taken for 1 epoch 120.05863451957703 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.7397\n",
            "Epoch 2 Batch 100 Loss 1.7423\n",
            "Epoch 2 Batch 200 Loss 1.7813\n",
            "Epoch 2 Batch 300 Loss 1.6997\n",
            "Epoch 2 Batch 400 Loss 1.6736\n",
            "Epoch 2 Loss 1.7331\n",
            "Time taken for 1 epoch 43.12205672264099 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.5814\n",
            "Epoch 3 Batch 100 Loss 1.6199\n",
            "Epoch 3 Batch 200 Loss 1.6756\n",
            "Epoch 3 Batch 300 Loss 1.5605\n",
            "Epoch 3 Batch 400 Loss 1.3981\n",
            "Epoch 3 Loss 1.6216\n",
            "Time taken for 1 epoch 40.73565983772278 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6252\n",
            "Epoch 4 Batch 100 Loss 1.5812\n",
            "Epoch 4 Batch 200 Loss 1.5103\n",
            "Epoch 4 Batch 300 Loss 1.5693\n",
            "Epoch 4 Batch 400 Loss 1.5740\n",
            "Epoch 4 Loss 1.5263\n",
            "Time taken for 1 epoch 40.38520789146423 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5224\n",
            "Epoch 5 Batch 100 Loss 1.3439\n",
            "Epoch 5 Batch 200 Loss 1.5436\n",
            "Epoch 5 Batch 300 Loss 1.4501\n",
            "Epoch 5 Batch 400 Loss 1.3645\n",
            "Epoch 5 Loss 1.4350\n",
            "Time taken for 1 epoch 41.322548389434814 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3480\n",
            "Epoch 6 Batch 100 Loss 1.2800\n",
            "Epoch 6 Batch 200 Loss 1.3432\n",
            "Epoch 6 Batch 300 Loss 1.4655\n",
            "Epoch 6 Batch 400 Loss 1.4706\n",
            "Epoch 6 Loss 1.3473\n",
            "Time taken for 1 epoch 41.119227170944214 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2538\n",
            "Epoch 7 Batch 100 Loss 1.2659\n",
            "Epoch 7 Batch 200 Loss 1.2849\n",
            "Epoch 7 Batch 300 Loss 1.3207\n",
            "Epoch 7 Batch 400 Loss 1.3109\n",
            "Epoch 7 Loss 1.2640\n",
            "Time taken for 1 epoch 40.39493775367737 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.1911\n",
            "Epoch 8 Batch 100 Loss 1.0739\n",
            "Epoch 8 Batch 200 Loss 1.2652\n",
            "Epoch 8 Batch 300 Loss 1.2978\n",
            "Epoch 8 Batch 400 Loss 1.2283\n",
            "Epoch 8 Loss 1.1838\n",
            "Time taken for 1 epoch 43.51608180999756 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0850\n",
            "Epoch 9 Batch 100 Loss 1.1116\n",
            "Epoch 9 Batch 200 Loss 1.0754\n",
            "Epoch 9 Batch 300 Loss 1.1396\n",
            "Epoch 9 Batch 400 Loss 1.1174\n",
            "Epoch 9 Loss 1.1058\n",
            "Time taken for 1 epoch 40.26758313179016 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0268\n",
            "Epoch 10 Batch 100 Loss 1.1381\n",
            "Epoch 10 Batch 200 Loss 1.0029\n",
            "Epoch 10 Batch 300 Loss 1.0150\n",
            "Epoch 10 Batch 400 Loss 1.0499\n",
            "Epoch 10 Loss 1.0303\n",
            "Time taken for 1 epoch 46.294575691223145 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.8870\n",
            "Epoch 11 Batch 100 Loss 0.8849\n",
            "Epoch 11 Batch 200 Loss 1.0139\n",
            "Epoch 11 Batch 300 Loss 1.0096\n",
            "Epoch 11 Batch 400 Loss 1.0161\n",
            "Epoch 11 Loss 0.9590\n",
            "Time taken for 1 epoch 40.886852741241455 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.8701\n",
            "Epoch 12 Batch 100 Loss 0.9325\n",
            "Epoch 12 Batch 200 Loss 0.9895\n",
            "Epoch 12 Batch 300 Loss 0.9413\n",
            "Epoch 12 Batch 400 Loss 1.0509\n",
            "Epoch 12 Loss 0.8907\n",
            "Time taken for 1 epoch 40.69263315200806 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.9540\n",
            "Epoch 13 Batch 100 Loss 0.8772\n",
            "Epoch 13 Batch 200 Loss 0.8444\n",
            "Epoch 13 Batch 300 Loss 0.7984\n",
            "Epoch 13 Batch 400 Loss 0.8650\n",
            "Epoch 13 Loss 0.8273\n",
            "Time taken for 1 epoch 40.606189250946045 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.6987\n",
            "Epoch 14 Batch 100 Loss 0.8221\n",
            "Epoch 14 Batch 200 Loss 0.7401\n",
            "Epoch 14 Batch 300 Loss 0.7161\n",
            "Epoch 14 Batch 400 Loss 0.8025\n",
            "Epoch 14 Loss 0.7694\n",
            "Time taken for 1 epoch 40.899418592453 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.6899\n",
            "Epoch 15 Batch 100 Loss 0.6981\n",
            "Epoch 15 Batch 200 Loss 0.7499\n",
            "Epoch 15 Batch 300 Loss 0.6732\n",
            "Epoch 15 Batch 400 Loss 0.6774\n",
            "Epoch 15 Loss 0.7142\n",
            "Time taken for 1 epoch 40.07269740104675 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.6018\n",
            "Epoch 16 Batch 100 Loss 0.6110\n",
            "Epoch 16 Batch 200 Loss 0.6857\n",
            "Epoch 16 Batch 300 Loss 0.7249\n",
            "Epoch 16 Batch 400 Loss 0.7151\n",
            "Epoch 16 Loss 0.6672\n",
            "Time taken for 1 epoch 45.88385272026062 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.5980\n",
            "Epoch 17 Batch 100 Loss 0.5933\n",
            "Epoch 17 Batch 200 Loss 0.6671\n",
            "Epoch 17 Batch 300 Loss 0.5593\n",
            "Epoch 17 Batch 400 Loss 0.6353\n",
            "Epoch 17 Loss 0.6220\n",
            "Time taken for 1 epoch 40.2308132648468 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.5424\n",
            "Epoch 18 Batch 100 Loss 0.5734\n",
            "Epoch 18 Batch 200 Loss 0.5690\n",
            "Epoch 18 Batch 300 Loss 0.6281\n",
            "Epoch 18 Batch 400 Loss 0.5253\n",
            "Epoch 18 Loss 0.5804\n",
            "Time taken for 1 epoch 41.4371554851532 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.4819\n",
            "Epoch 19 Batch 100 Loss 0.4451\n",
            "Epoch 19 Batch 200 Loss 0.5314\n",
            "Epoch 19 Batch 300 Loss 0.5820\n",
            "Epoch 19 Batch 400 Loss 0.5246\n",
            "Epoch 19 Loss 0.5428\n",
            "Time taken for 1 epoch 40.237309217453 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4690\n",
            "Epoch 20 Batch 100 Loss 0.4865\n",
            "Epoch 20 Batch 200 Loss 0.4965\n",
            "Epoch 20 Batch 300 Loss 0.5659\n",
            "Epoch 20 Batch 400 Loss 0.5089\n",
            "Epoch 20 Loss 0.5099\n",
            "Time taken for 1 epoch 45.196767807006836 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wy6eZorfWgdU"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WO0LOpZxWgdV"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "75rRvv2vWgdV",
        "outputId": "8e06f1fd-7eed-4af8-f0a3-4ca151addc66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f0e3651b6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nBYvmjhHWgdV",
        "outputId": "4f6797fa-6bf4-41d9-e295-46aa2bcc5729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: the world is bipolar .\n",
            "Predicted translation: दुनिया में कैंसर है, <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'The world is bipolar.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}